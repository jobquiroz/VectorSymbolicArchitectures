{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Semantic Similarity\n",
    "\n",
    "This program measure the similarity between pairs of words from the McRae's dataset. First it uses the HD Computing approach and then compares it with similarity metrics from the NLTK library.\n",
    "\n",
    "### Importing libraries and HD computing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "\n",
    "%run HDComputing_basics.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TranslateFeats(ListFeat):\n",
    "    \"It receives a list of features such as ['is_blue', 'is_rectangular'] and it returns: [['color','blue'], ['shape','rectangular']\"\n",
    "    # Dataframe for excel document\n",
    "    df = pd.read_excel('../McRaedataset/FEATS_brm.xlsx')\n",
    "    ListPairs = []\n",
    "    for feat in ListFeat:\n",
    "        # Row for feature...\n",
    "        row = df.loc[df['Feature'] == feat]       \n",
    "        # Look for values in vec_feat and vec_value\n",
    "        ListPairs.append([str(row['feat_name'].tolist()[0]), str(row['feat_value'].tolist()[0])])       \n",
    "    return ListPairs\n",
    "\n",
    "def ReadDefinitions():\n",
    "    \"Given an xlsx file it returns all the concepts feature values as they appear in the original dataset\"\n",
    "    #Dataframe for excel document\n",
    "    df = pd.read_excel('../McRaeDataset/CONCS_FEATS_concstats_brm.xlsx') #MINI_\n",
    "    #Create a list with all concept names\n",
    "    names = set(df['Concept'])\n",
    "    # Extract list of features for each name\n",
    "    Concepts = []\n",
    "    for n in names:\n",
    "        row = df.loc[df['Concept'] == n]\n",
    "        Concepts.append([str(n), map(str,list(row['Feature']))])\n",
    "    return Concepts\n",
    "\n",
    "def ClosestConcepts (concept, nc):\n",
    "    \"Given a concept label this function reads the distance matrix from McRae's and returns the 'nc' closests concepts in a list\"\n",
    "    # Excel document to data frame...\n",
    "    df = pd.read_excel('../McRaeDataset/cos_matrix_brm_IFR.xlsx')\n",
    "    ordered = df.sort_values(by=concept, ascending=False)[['CONCEPT', concept]]\n",
    "    \n",
    "    L1 = list(ordered['CONCEPT'][0:nc])\n",
    "    L1 = map(str, L1)\n",
    "    L2 = zip(L1,list(ordered[concept][0:nc]))\n",
    "    L2 = map(list, L2)\n",
    "    \n",
    "    return L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating definitions dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDictionary():\n",
    "    global Dict_defs\n",
    "    data = ReadDefinitions()\n",
    "    for concept in data:\n",
    "        Dict_defs[concept[0]] = TranslateFeats(concept[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing ID vectors into memory\n",
    "\n",
    "### Memory functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_list (L):\n",
    "    \"Recursive function that flats a list of lists (at any level)\"\n",
    "    if L == []:\n",
    "        return L\n",
    "    if type(L[0]) is list:\n",
    "        return flat_list(L[0]) + flat_list(L[1:])\n",
    "    return L[:1] + flat_list(L[1:])\n",
    "\n",
    "def SaveConcepts(Dic):\n",
    "    \"\"\"Given a definitions dictionary it stores in memory the entire set of concepts in the dictionary (including feature vectors)\"\"\"\n",
    "    keys = Dic.keys()\n",
    "    vals = Dic.values()\n",
    "    all_concepts = list(set(flat_list(vals) + keys))\n",
    "    # Process for storing list of concepts in memory\n",
    "    for concept in all_concepts:\n",
    "        HDvector(N,concept) #This creates an object and store it in memory\n",
    "        \n",
    "def FeatureVectors(Dic):\n",
    "    \"It extract from the definition dictionary all the feature type vectors ('is','has','color', etc...)\"\n",
    "    global feature_vectors\n",
    "    featt = []\n",
    "    vals = Dic.values()\n",
    "    for l in vals:\n",
    "        for p in l:\n",
    "            featt.append(p[0])\n",
    "    feature_vectors = list(set(featt))\n",
    "    \n",
    "def CreateSemanticPointer (PairList):\n",
    "    \"Turns list as [[feat1,feat_val],[feat2,feat_val],[feat3,feat_val]] into vector feat1*feat_val + feat2*feat_val ...\"\n",
    "    vecs = []\n",
    "    for pair in PairList:\n",
    "        vecs.append(Dict[pair[0]] * Dict[pair[1]])\n",
    "    return ADD(vecs)\n",
    "\n",
    "def SaveDefinitions(Dic):\n",
    "    \"\"\"Given the definitions dictionary, and having all its concepts previously stored in memory, this functions\n",
    "       creates a definition vector (semantic pointer) using HD operations and assign it as a pointer to an \n",
    "       object vector (ID vector).\"\"\"\n",
    "    global feature_vectors\n",
    "    # Going through all elements in dictionary\n",
    "    for key, value in Dic.iteritems():\n",
    "        Dict[key].setPointer(CreateSemanticPointer(value))\n",
    "        \n",
    "def NormalizeHammDist (Dist_list):\n",
    "    \"Given a distance list of the form [['name', dist], ['name', dist], ... ], it normalize each distance and return a list with the same form\"\n",
    "    for i in range(len(Dist_list)):\n",
    "        Dist_list[i][1] = round( 1. - Dist_list[i][1] / float(N / 2), 3 )\n",
    "    return Dist_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of initialization\n",
      "End of encoding\n"
     ]
    }
   ],
   "source": [
    "def Init_mem():\n",
    "    init()\n",
    "    thr = 0.4 * N\n",
    "    # Read dataset and create definition dictionary\n",
    "    CreateDictionary()\n",
    "    # Feature vectors\n",
    "    FeatureVectors(Dict_defs)\n",
    "    # Save concepts into memory (ID vectors)\n",
    "    SaveConcepts(Dict_defs)\n",
    "    # Associate definitions to concepts into memory (SP vectors)\n",
    "    SaveDefinitions(Dict_defs)\n",
    "    print \"End of encoding\"\n",
    "\n",
    "Init_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting list of closest vectors (HDcomputing and from Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest concepts using semantic features\n",
      "\n",
      "Number of concepts:  8\n",
      "\n",
      "\n",
      "Closest concepts to ' airplane ' definition:  [['airplane', 1.0], ['jet', 0.523], ['train', 0.243], ['helicopter', 0.227], ['rocket', 0.216], ['trolley', 0.185], ['scooter', 0.175], ['hornet', 0.173]]\n",
      "\n",
      "\n",
      "Closest concepts to ' airplane ' (from Dataset):  [['airplane', 1.0], ['jet', 0.775], ['housefly', 0.476], ['falcon', 0.471], ['moth', 0.469], ['hornet', 0.465], ['butterfly', 0.452], ['pigeon', 0.446]]\n"
     ]
    }
   ],
   "source": [
    "test_concept = 'airplane'\n",
    "num_concepts_1 = 8 #20\n",
    "\n",
    "# Asking closest concept of another concept's definition...\n",
    "HDC_sim = HDvector.getLabelSP(Dict[test_concept].getPointer())[:num_concepts_1]\n",
    "# Normalizing\n",
    "HDC_sim = NormalizeHammDist(HDC_sim)\n",
    "DatSet_sim = ClosestConcepts(test_concept, num_concepts_1)\n",
    "\n",
    "print \"Closest concepts using semantic features\"\n",
    "print \"\\nNumber of concepts: \", num_concepts_1\n",
    "\n",
    "print \"\\n\\nClosest concepts to '\", test_concept,\"' definition: \", HDC_sim\n",
    "print \"\\n\\nClosest concepts to '\", test_concept,\"' (from Dataset): \", DatSet_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Similarity using NLTK library\n",
    "\n",
    "### Auxiliar functions for similarity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concepts_list ():\n",
    "    \"Returns a list of strings: the names of the concepts\"\n",
    "    df = pd.read_excel('../McRaedataset/CONCS_Synset_brm.xlsx')\n",
    "    return map(str, list(df['Concept']))\n",
    "    \n",
    "def get_synset (concept):\n",
    "    \"Given a concept name (string) it returns its synset (string)\"\n",
    "    # Dataframe for excel document\n",
    "    df = pd.read_excel('../McRaedataset/CONCS_Synset_brm.xlsx')\n",
    "    row = df.loc[df['Concept'] == concept]\n",
    "    return str(list(row['Synset'] )[0])\n",
    "\n",
    "def Apply_sim_metric ( similarity_metric, num, in_concept, corpus = None):\n",
    "    \"Given a similarity_metric function it returns a list of the num closest concepts to 'concept'\"\n",
    "    dist_list = []\n",
    "    for c in Concepts:\n",
    "        c_synset = wn.synset( get_synset(c) )\n",
    "        if corpus:\n",
    "            dist_list.append([c, round(similarity_metric(in_concept, c_synset, corpus), 3) ])\n",
    "        else:\n",
    "            dist_list.append([c, round(similarity_metric(in_concept, c_synset), 3) ])\n",
    "    return sorted(dist_list, key = lambda r : r[1], reverse = True ) [:num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting list of closest vectors (Path-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "METRICS BASED ON PATH\n",
      "\n",
      "Number of concepts:  15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-128-6a5184f98eac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# PATH SIMILARITY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mPath_sim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mApply_sim_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_similarity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_concepts_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcept\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"\\nWordNet path similarity: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPath_sim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-121-ad56d08e9e06>\u001b[0m in \u001b[0;36mApply_sim_metric\u001b[1;34m(similarity_metric, num, in_concept, corpus)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mdist_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mConcepts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mc_synset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynset\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mget_synset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mdist_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimilarity_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_concept\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_synset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-121-ad56d08e9e06>\u001b[0m in \u001b[0;36mget_synset\u001b[1;34m(concept)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;34m\"Given a concept name (string) it returns its synset (string)\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Dataframe for excel document\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../McRaedataset/CONCS_Synset_brm.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Concept'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mconcept\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Synset'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jobqu\\Anaconda2\\lib\\site-packages\\pandas\\util\\_decorators.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jobqu\\Anaconda2\\lib\\site-packages\\pandas\\io\\excel.pyc\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, skiprows, skip_footer, index_col, names, usecols, parse_dates, date_parser, na_values, thousands, convert_float, converters, dtype, true_values, false_values, engine, squeeze, **kwds)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     return io._parse_excel(\n",
      "\u001b[1;32mC:\\Users\\jobqu\\Anaconda2\\lib\\site-packages\\pandas\\io\\excel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, io, **kwds)\u001b[0m\n\u001b[0;32m    292\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             raise ValueError('Must explicitly set engine if not passing in'\n",
      "\u001b[1;32mC:\\Users\\jobqu\\Anaconda2\\lib\\site-packages\\xlrd\\__init__.pyc\u001b[0m in \u001b[0;36mopen_workbook\u001b[1;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[0;32m    141\u001b[0m                 \u001b[0mformatting_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformatting_info\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \u001b[0mon_demand\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_demand\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m                 \u001b[0mragged_rows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mragged_rows\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m                 )\n\u001b[0;32m    145\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mbk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jobqu\\Anaconda2\\lib\\site-packages\\xlrd\\xlsx.pyc\u001b[0m in \u001b[0;36mopen_workbook_2007_xml\u001b[1;34m(zf, component_names, logfile, verbosity, use_mmap, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[0;32m    835\u001b[0m         \u001b[0mx12sheet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX12Sheet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msheet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    836\u001b[0m         \u001b[0mheading\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Sheet %r (sheetx=%d) from %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msheet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msheetx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 837\u001b[1;33m         \u001b[0mx12sheet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzflo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheading\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    838\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mzflo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jobqu\\Anaconda2\\lib\\site-packages\\xlrd\\xlsx.pyc\u001b[0m in \u001b[0;36mown_process_stream\u001b[1;34m(self, stream, heading)\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mET\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mrow_tag\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 548\u001b[1;33m                 \u001b[0mself_do_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    549\u001b[0m                 \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# destroy all child elements (cells)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mU_SSML12\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"dimension\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jobqu\\Anaconda2\\lib\\site-packages\\xlrd\\xlsx.pyc\u001b[0m in \u001b[0;36mdo_row\u001b[1;34m(self, row_elem)\u001b[0m\n\u001b[0;32m    719\u001b[0m                         \u001b[0mformula\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcooked_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchild\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 721\u001b[1;33m                         \u001b[0mbad_child_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild_tag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    722\u001b[0m                 \u001b[1;31m# assert tvalue is not None and formula is not None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m                 \u001b[1;31m# Yuk. Fails with file created by gnumeric -- no tvalue!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# List of concepts\n",
    "Concepts = get_concepts_list() \n",
    "\n",
    "#Input concept\n",
    "concept = wn.synset( get_synset(\"airplane\") )\n",
    "\n",
    "# Number of closest concepts to take into account\n",
    "num_concepts_2 = 15 #20\n",
    "\n",
    "print \"\\nMETRICS BASED ON PATH\"\n",
    "print \"\\nNumber of concepts: \", num_concepts_2\n",
    "\n",
    "# PATH SIMILARITY\n",
    "Path_sim = Apply_sim_metric(wn.path_similarity, num_concepts_2, concept)\n",
    "print \"\\nWordNet path similarity: \", Path_sim\n",
    "\n",
    "# Leacock-Chodorow similarity\n",
    "LC_sim = Apply_sim_metric(wn.lch_similarity, num_concepts_2, concept ) \n",
    "print \"\\n\\nLeacock-Chodorow similarity: \", LC_sim\n",
    "\n",
    "# Wu-Palmer Similarity\n",
    "WUP_sim = Apply_sim_metric(wn.wup_similarity, num_concepts_2, concept )\n",
    "print \"\\n\\nWu-Palmer similarity: \", WUP_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting list of closest vectors (Information content-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS BASED ON INFORMATION CONTENT\n",
      "\n",
      "Number of concepts:  15\n",
      "\n",
      "Resnick similarity (brown_ic):  [['airplane', 8.871], ['jet', 8.871], ['helicopter', 8.643], ['boat', 7.221], ['canoe', 7.221], ['sailboat', 7.221], ['ship', 7.221], ['submarine', 7.221], ['yacht', 7.221], ['ambulance', 5.922], ['bike', 5.922], ['buggy', 5.922], ['car', 5.922], ['cart', 5.922], ['dunebuggy', 5.922]]\n",
      "\n",
      "\n",
      "Jiang-Conrath similarity (brown_ic):  [['airplane', 1e+300], ['jet', 0.477], ['ship', 0.363], ['boat', 0.323], ['submarine', 0.219], ['car', 0.217], ['helicopter', 0.202], ['truck', 0.178], ['canoe', 0.165], ['bike', 0.159], ['missile', 0.158], ['train', 0.148], ['van', 0.141], ['yacht', 0.141], ['wagon', 0.139]]\n",
      "\n",
      "\n",
      "Lin similarity (brown_ic):  [['airplane', 1.0], ['jet', 0.894], ['ship', 0.84], ['boat', 0.823], ['helicopter', 0.777], ['submarine', 0.759], ['car', 0.719], ['canoe', 0.704], ['truck', 0.679], ['yacht', 0.67], ['sailboat', 0.658], ['bike', 0.654], ['missile', 0.652], ['train', 0.631], ['van', 0.625]]\n"
     ]
    }
   ],
   "source": [
    "## Corpus Information content...\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "#semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "\n",
    "print \"METRICS BASED ON INFORMATION CONTENT\"\n",
    "print \"\\nNumber of concepts: \", num_concepts_2\n",
    "\n",
    "# Resnik similarity.\n",
    "Res_sim = Apply_sim_metric(wn.res_similarity, num_concepts_2, concept, brown_ic)\n",
    "print \"\\nResnick similarity (brown_ic): \", Res_sim\n",
    "#Res_sim2 = Apply_sim_metric(wn.res_similarity, 20, concept, semcor_ic)\n",
    "#print \"\\nResnick similarity (semcor_ic): \", Res_sim2\n",
    "\n",
    "# Jiang-Conrath similarity\n",
    "JC_sim = Apply_sim_metric(wn.jcn_similarity, num_concepts_2, concept, brown_ic)\n",
    "print \"\\n\\nJiang-Conrath similarity (brown_ic): \", JC_sim\n",
    "#JC_sim2 = Apply_sim_metric(wn.jcn_similarity, 20, concept, semcor_ic)\n",
    "#print \"\\nJiang-Conrath similarity (semcor_ic): \", JC_sim2\n",
    "\n",
    "# Lin similarity\n",
    "Lin_sim = Apply_sim_metric(wn.lin_similarity, num_concepts_2, concept, brown_ic)\n",
    "print \"\\n\\nLin similarity (brown_ic): \", Lin_sim\n",
    "#Lin_sim2 = Apply_sim_metric(wn.lin_similarity, 20, concept, semcor_ic)\n",
    "#print \"\\nLin similarity (semcor_ic): \", Lin_sim2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersections and unions\n",
    "In this following cell we obtain the intersection sets for the McRae and HDcomputing similarity name list. The goal is to compare which method is more similar to each of the similarity metrics.\n",
    "The union sets is going to be used for creating Surveys to be applied on humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Ultimate Union** lenght:  18 \n",
      "\n",
      "set(['canoe', 'skateboard', 'sled', 'missile', 'airplane', 'ship', 'boat', 'jet', 'sailboat', 'scooter', 'yacht', 'tank_(army)', 'submarine', 'bike', 'car', 'sleigh', 'helicopter', 'hornet'])\n"
     ]
    }
   ],
   "source": [
    "# Creating name sets\n",
    "HDC_names = set([x[0] for x in HDC_sim])\n",
    "DatSet_names = set([x[0] for x in DatSet_sim])\n",
    "Path_names = set([x[0] for x in Path_sim])\n",
    "LC_names = set([x[0] for x in LC_sim])\n",
    "WUP_names = set([x[0] for x in WUP_sim])\n",
    "Res_names = set([x[0] for x in Res_sim])\n",
    "JC_names = set([x[0] for x in JC_sim])\n",
    "Lin_names = set([x[0] for x in Lin_sim])\n",
    "\n",
    "# Intersection between HDC and Dataset\n",
    "union_1 = HDC_names.intersection(DatSet_names)\n",
    "\n",
    "# Union of intersecion Path and intersection IC...\n",
    "PathInt = Path_names.intersection(LC_names, WUP_names)\n",
    "ICInt = Res_names.intersection(JC_names, Lin_names)\n",
    "union_2 = PathInt.union(ICInt)\n",
    "\n",
    "# Intersection between all NLTK metrics\n",
    "union_3 = PathInt.intersection(ICInt)\n",
    "\n",
    "# Intersection all NLTLK metrics and HDC and Dataset\n",
    "union_4 = HDC_names.intersection(PathInt, ICInt)\n",
    "union_5 = DatSet_names.intersection(PathInt, ICInt)\n",
    "union_6 = HDC_names.intersection(DatSet_names, PathInt, ICInt)\n",
    "\n",
    "# Ultimate union...\n",
    "ult_union = set.union( union_1, union_2, union_3, union_4, union_5, union_6 )\n",
    "print \"**Ultimate Union** lenght: \", len(ult_union), \"\\n\\n\", ult_union\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous cell... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*1*HDC and Dataset:  set(['airplane', 'hornet', 'jet'])\n",
      "\n",
      "\n",
      "HDC & NLTK similarity metrics\n",
      "\n",
      "set(['helicopter', 'scooter', 'airplane', 'jet'])\n",
      "\n",
      "set(['helicopter', 'scooter', 'airplane', 'jet'])\n",
      "\n",
      "set(['helicopter', 'scooter', 'airplane', 'jet'])\n",
      "\n",
      "set(['helicopter', 'airplane', 'jet'])\n",
      "\n",
      "set(['helicopter', 'airplane', 'train', 'jet'])\n",
      "\n",
      "set(['helicopter', 'airplane', 'train', 'jet'])\n",
      "\n",
      "\n",
      "Dataset & NLTK similarity metrics\n",
      "\n",
      "set(['airplane', 'jet'])\n",
      "\n",
      "set(['airplane', 'jet'])\n",
      "\n",
      "set(['airplane', 'jet'])\n",
      "\n",
      "set(['airplane', 'jet'])\n",
      "\n",
      "set(['airplane', 'jet'])\n",
      "\n",
      "set(['airplane', 'jet'])\n",
      "\n",
      "\n",
      "Intersection Path based metrics\n",
      "\n",
      "set(['jet', 'sailboat', 'scooter', 'skateboard', 'yacht', 'sled', 'missile', 'tank_(army)', 'bike', 'sleigh', 'helicopter', 'ship', 'airplane', 'boat']) ... 93.3333333333 %\n",
      "\n",
      "\n",
      "Intersection IC based metrics\n",
      "\n",
      "set(['jet', 'canoe', 'car', 'yacht', 'submarine', 'bike', 'helicopter', 'airplane', 'ship', 'boat'])\n",
      "\n",
      "\n",
      "*2*Union of intersections\n",
      "\n",
      "set(['canoe', 'yacht', 'sled', 'missile', 'ship', 'airplane', 'boat', 'jet', 'sailboat', 'scooter', 'skateboard', 'tank_(army)', 'submarine', 'bike', 'car', 'sleigh', 'helicopter'])\n",
      "\n",
      "\n",
      "*3*Intersection between all NLTK metrics\n",
      "\n",
      "set(['jet', 'yacht', 'bike', 'helicopter', 'airplane', 'ship', 'boat'])\n",
      "\n",
      "\n",
      "*4*Intersection between NLTK metrics and HDC\n",
      "\n",
      "HDC:  set(['helicopter', 'airplane', 'jet'])\n",
      "\n",
      "*5*Dataset:  set(['airplane', 'jet'])\n",
      "\n",
      "*6*ALL:  set(['airplane', 'jet'])\n",
      "**Ultimate Union** lenght:  18 \n",
      "set(['canoe', 'skateboard', 'sled', 'missile', 'airplane', 'ship', 'boat', 'jet', 'sailboat', 'scooter', 'yacht', 'tank_(army)', 'submarine', 'bike', 'car', 'sleigh', 'helicopter', 'hornet'])\n",
      "\n",
      "\n",
      "Union lenght:  35 \n",
      "set(['rocket', 'canoe', 'moth', 'yacht', 'sled', 'housefly', 'airplane', 'ship', 'boat', 'trolley', 'van', 'jet', 'sailboat', 'scooter', 'pigeon', 'tank_(army)', 'submarine', 'bike', 'helicopter', 'butterfly', 'buggy', 'cart', 'missile', 'dunebuggy', 'train', 'falcon', 'wagon', 'tricycle', 'car', 'skateboard', 'truck', 'hornet', 'ambulance', 'sleigh', 'trailer'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Intersection between HDC and Dataset\n",
    "union_1 = HDC_names.intersection(DatSet_names)\n",
    "print \"*1*HDC and Dataset: \", union_1\n",
    "\n",
    "# Intersection between HDC and NLTK similarity metrics\n",
    "print \"\\n\\nHDC & NLTK similarity metrics\"\n",
    "for sett in [Path_names, LC_names, WUP_names, Res_names, JC_names, Lin_names]:\n",
    "    print \"\\n\", HDC_names.intersection(sett)\n",
    "    \n",
    "# Intersection between HDC and NLTK similarity metrics\n",
    "print \"\\n\\nDataset & NLTK similarity metrics\"\n",
    "for sett in [Path_names, LC_names, WUP_names, Res_names, JC_names, Lin_names]:\n",
    "    print \"\\n\", DatSet_names.intersection(sett)\n",
    "\n",
    "# Intersection between Path based metrics\n",
    "print \"\\n\\nIntersection Path based metrics\"\n",
    "PathInt = Path_names.intersection(LC_names, WUP_names)\n",
    "print \"\\n\", PathInt, \"...\", len(PathInt) / float( num_concepts_2 ) * 100, \"%\"\n",
    "\n",
    "# Intersection between IC based metrics\n",
    "print \"\\n\\nIntersection IC based metrics\"\n",
    "ICInt = Res_names.intersection(JC_names, Lin_names)\n",
    "print \"\\n\", ICInt\n",
    "\n",
    "# Union of intersecion Path and intersection IC...\n",
    "print \"\\n\\n*2*Union of intersections\"\n",
    "union_2 = PathInt.union(ICInt)\n",
    "print \"\\n\", union_2\n",
    "\n",
    "# Intersection between all NLTK metrics\n",
    "print \"\\n\\n*3*Intersection between all NLTK metrics\"\n",
    "union_3 = PathInt.intersection(ICInt)\n",
    "print \"\\n\", union_3\n",
    "\n",
    "# Intersection all NLTLK metrics and HDC and Dataset\n",
    "print \"\\n\\n*4*Intersection between NLTK metrics and HDC\"\n",
    "union_4 = HDC_names.intersection(PathInt, ICInt)\n",
    "print \"\\nHDC: \", union_4\n",
    "union_5 = DatSet_names.intersection(PathInt, ICInt)\n",
    "print \"\\n*5*Dataset: \", union_5\n",
    "union_6 = HDC_names.intersection(DatSet_names, PathInt, ICInt)\n",
    "print \"\\n*6*ALL: \", union_6\n",
    "\n",
    "\n",
    "# Ultimate union...\n",
    "ult_union = set.union( union_1, union_2, union_3, union_4, union_5, union_6 )\n",
    "print \"**Ultimate Union** lenght: \", len(ult_union), \"\\n\", ult_union\n",
    "\n",
    "# Union for all names...\n",
    "Union = set().union(HDC_names, DatSet_names, Path_names, LC_names, WUP_names, Res_names, JC_names, Lin_names)\n",
    "print \"\\n\\nUnion lenght: \", len(Union), \"\\n\", Union"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
