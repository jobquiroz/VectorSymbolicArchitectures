{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Semantic Similarity\n",
    "\n",
    "This program measure the similarity between pairs of words from the McRae's dataset. First it uses the HD Computing approach and then compares it with similarity metrics from the NLTK library.\n",
    "\n",
    "### Importing libraries and HD computing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "\n",
    "%run HDComputing_basics.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TranslateFeats(ListFeat):\n",
    "    \"It receives a list of features such as ['is_blue', 'is_rectangular'] and it returns: [['color','blue'], ['shape','rectangular']\"\n",
    "    # Dataframe for excel document\n",
    "    df = pd.read_excel('FEATS_brm.xlsx') #../McRaedataset/FEATS_brm.xlsx')\n",
    "    ListPairs = []\n",
    "    for feat in ListFeat:\n",
    "        # Row for feature...\n",
    "        row = df.loc[df['Feature'] == feat]       \n",
    "        # Look for values in vec_feat and vec_value\n",
    "        ListPairs.append([str(row['feat_name'].tolist()[0]), str(row['feat_value'].tolist()[0])])       \n",
    "    return ListPairs\n",
    "\n",
    "def ReadDefinitions():\n",
    "    \"Given an xlsx file it returns all the concepts feature values as they appear in the original dataset\"\n",
    "    #Dataframe for excel document\n",
    "    df = pd.read_excel('CONCS_FEATS_concstats_brm.xlsx') #../McRaeDataset/CONCS_FEATS_concstats_brm.xlsx') #MINI_\n",
    "    #Create a list with all concept names\n",
    "    names = set(df['Concept'])\n",
    "    # Extract list of features for each name\n",
    "    Concepts = []\n",
    "    for n in names:\n",
    "        row = df.loc[df['Concept'] == n]\n",
    "        Concepts.append([str(n), map(str,list(row['Feature']))])\n",
    "    return Concepts\n",
    "\n",
    "def ClosestConcepts (concept, nc):\n",
    "    \"Given a concept label this function reads the distance matrix from McRae's and returns the 'nc' closests concepts in a list\"\n",
    "    # Excel document to data frame...\n",
    "    df = pd.read_excel('cos_matrix_brm_IFR.xlsx') #../McRaeDataset/cos_matrix_brm_IFR.xlsx')\n",
    "    ordered = df.sort_values(by=concept, ascending=False)[['CONCEPT', concept]]\n",
    "    \n",
    "    L1 = list(ordered['CONCEPT'][0:nc])\n",
    "    L1 = map(str, L1)\n",
    "    L2 = zip(L1,list(ordered[concept][0:nc]))\n",
    "    L2 = map(list, L2)\n",
    "    \n",
    "    return L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating definitions dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDictionary():\n",
    "    global Dict_defs\n",
    "    data = ReadDefinitions()\n",
    "    for concept in data:\n",
    "        Dict_defs[concept[0]] = TranslateFeats(concept[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing ID vectors into memory\n",
    "\n",
    "### Memory functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_list (L):\n",
    "    \"Recursive function that flats a list of lists (at any level)\"\n",
    "    if L == []:\n",
    "        return L\n",
    "    if type(L[0]) is list:\n",
    "        return flat_list(L[0]) + flat_list(L[1:])\n",
    "    return L[:1] + flat_list(L[1:])\n",
    "\n",
    "def SaveConcepts(Dic):\n",
    "    \"\"\"Given a definitions dictionary it stores in memory the entire set of concepts in the dictionary (including feature vectors)\"\"\"\n",
    "    keys = Dic.keys()\n",
    "    vals = Dic.values()\n",
    "    all_concepts = list(set(flat_list(vals) + keys))\n",
    "    # Process for storing list of concepts in memory\n",
    "    for concept in all_concepts:\n",
    "        HDvector(N,concept) #This creates an object and store it in memory\n",
    "        \n",
    "def FeatureVectors(Dic):\n",
    "    \"It extract from the definition dictionary all the feature type vectors ('is','has','color', etc...)\"\n",
    "    global feature_vectors\n",
    "    featt = []\n",
    "    vals = Dic.values()\n",
    "    for l in vals:\n",
    "        for p in l:\n",
    "            featt.append(p[0])\n",
    "    feature_vectors = list(set(featt))\n",
    "    \n",
    "def CreateSemanticPointer (PairList):\n",
    "    \"Turns list as [[feat1,feat_val],[feat2,feat_val],[feat3,feat_val]] into vector feat1*feat_val + feat2*feat_val ...\"\n",
    "    vecs = []\n",
    "    for pair in PairList:\n",
    "        vecs.append(Dict[pair[0]] * Dict[pair[1]])\n",
    "    return ADD(vecs)\n",
    "\n",
    "def SaveDefinitions(Dic):\n",
    "    \"\"\"Given the definitions dictionary, and having all its concepts previously stored in memory, this functions\n",
    "       creates a definition vector (semantic pointer) using HD operations and assign it as a pointer to an \n",
    "       object vector (ID vector).\"\"\"\n",
    "    global feature_vectors\n",
    "    # Going through all elements in dictionary\n",
    "    for key, value in Dic.iteritems():\n",
    "        Dict[key].setPointer(CreateSemanticPointer(value))\n",
    "        \n",
    "def NormalizeHammDist (Dist_list):\n",
    "    \"Given a distance list of the form [['name', dist], ['name', dist], ... ], it normalize each distance and return a list with the same form\"\n",
    "    for i in range(len(Dist_list)):\n",
    "        Dist_list[i][1] = round( 1. - Dist_list[i][1] / float(N / 2), 3 )\n",
    "    return Dist_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of initialization\n",
      "End of encoding\n"
     ]
    }
   ],
   "source": [
    "def Init_mem():\n",
    "    init()\n",
    "    thr = 0.4 * N\n",
    "    # Read dataset and create definition dictionary\n",
    "    CreateDictionary()\n",
    "    # Feature vectors\n",
    "    FeatureVectors(Dict_defs)\n",
    "    # Save concepts into memory (ID vectors)\n",
    "    SaveConcepts(Dict_defs)\n",
    "    # Associate definitions to concepts into memory (SP vectors)\n",
    "    SaveDefinitions(Dict_defs)\n",
    "    print \"End of encoding\"\n",
    "\n",
    "Init_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting list of closest vectors (HDcomputing and from Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sin normalizar [['bed', 0], ['sofa', 4044], ['couch', 4172], ['nightgown', 4209], ['pajamas', 4258], ['pillow', 4410], ['cushion', 4432], ['robe', 4447], ['mink_(coat)', 4498]]\n",
      "Closest concepts using semantic features\n",
      "\n",
      "Number of concepts:  20\n",
      "\n",
      "\n",
      "Closest concepts to ' bed ' definition:  [['bed', 1.0], ['sofa', 0.191], ['couch', 0.166], ['nightgown', 0.158], ['pajamas', 0.148], ['pillow', 0.118], ['cushion', 0.114], ['robe', 0.111], ['mink_(coat)', 0.1]]\n",
      "\n",
      "\n",
      "Closest concepts to ' bed ' (from Dataset):  [['bed', 1.0], ['pillow', 0.374], ['sofa', 0.323], ['couch', 0.322], ['cushion', 0.284], ['mink_(coat)', 0.158], ['nightgown', 0.152], ['pajamas', 0.149], ['robe', 0.146], ['slippers', 0.145], ['dresser', 0.121], ['earmuffs', 0.105], ['carpet', 0.098], ['camisole', 0.087], ['chair', 0.082], ['bureau', 0.079], ['parka', 0.077], ['jeans', 0.073], ['banana', 0.067], ['mirror', 0.066]]\n"
     ]
    }
   ],
   "source": [
    "test_concept = 'bed'\n",
    "num_concepts_1 = 20 #20\n",
    "\n",
    "# Asking closest concept of another concept's definition...\n",
    "HDC_sim = HDvector.getLabelSP(Dict[test_concept].getPointer())[:num_concepts_1]\n",
    "print \"Sin normalizar\", HDC_sim\n",
    "# Normalizing\n",
    "HDC_sim = NormalizeHammDist(HDC_sim)\n",
    "DatSet_sim = ClosestConcepts(test_concept, num_concepts_1)\n",
    "\n",
    "print \"Closest concepts using semantic features\"\n",
    "print \"\\nNumber of concepts: \", num_concepts_1\n",
    "\n",
    "print \"\\n\\nClosest concepts to '\", test_concept,\"' definition: \", HDC_sim\n",
    "print \"\\n\\nClosest concepts to '\", test_concept,\"' (from Dataset): \", DatSet_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Similarity using NLTK library\n",
    "\n",
    "### Auxiliar functions for similarity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concepts_list ():\n",
    "    \"Returns a list of strings: the names of the concepts\"\n",
    "    df = pd.read_excel('CONCS_Synset_brm.xlsx')\n",
    "    return map(str, list(df['Concept']))\n",
    "    \n",
    "def get_synset (concept):\n",
    "    \"Given a concept name (string) it returns its synset (string)\"\n",
    "    # Dataframe for excel document\n",
    "    df = pd.read_excel('CONCS_Synset_brm.xlsx')\n",
    "    row = df.loc[df['Concept'] == concept]\n",
    "    return str(list(row['Synset'] )[0])\n",
    "\n",
    "def Apply_sim_metric ( similarity_metric, num, in_concept, corpus = None):\n",
    "    \"Given a similarity_metric function it returns a list of the num closest concepts to 'concept'\"\n",
    "    dist_list = []\n",
    "    for c in Concepts:\n",
    "        c_synset = wn.synset( get_synset(c) )\n",
    "        if corpus:\n",
    "            dist_list.append([c, round(similarity_metric(in_concept, c_synset, corpus), 3) ])\n",
    "        else:\n",
    "            dist_list.append([c, round(similarity_metric(in_concept, c_synset), 3) ])\n",
    "    return sorted(dist_list, key = lambda r : r[1], reverse = True ) [:num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting list of closest vectors (Path-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "METRICS BASED ON PATH\n",
      "\n",
      "Number of concepts:  15\n",
      "\n",
      "WordNet path similarity:  [['airplane', 1.0], ['jet', 0.5], ['helicopter', 0.333], ['boat', 0.167], ['ship', 0.167], ['sled', 0.167], ['sleigh', 0.167], ['yacht', 0.167], ['bike', 0.143], ['missile', 0.143], ['sailboat', 0.143], ['scooter', 0.143], ['skateboard', 0.143], ['tank_(army)', 0.143], ['trailer', 0.143]]\n",
      "\n",
      "\n",
      "Leacock-Chodorow similarity:  [['airplane', 3.638], ['jet', 2.944], ['helicopter', 2.539], ['boat', 1.846], ['ship', 1.846], ['sled', 1.846], ['sleigh', 1.846], ['yacht', 1.846], ['bike', 1.692], ['missile', 1.692], ['sailboat', 1.692], ['scooter', 1.692], ['skateboard', 1.692], ['tank_(army)', 1.692], ['trailer', 1.692]]\n",
      "\n",
      "\n",
      "Wu-Palmer similarity:  [['airplane', 1.0], ['jet', 0.96], ['helicopter', 0.917], ['boat', 0.783], ['ship', 0.783], ['yacht', 0.783], ['sled', 0.762], ['sleigh', 0.762], ['sailboat', 0.75], ['bike', 0.727], ['missile', 0.727], ['scooter', 0.727], ['skateboard', 0.727], ['tank_(army)', 0.727], ['tricycle', 0.727]]\n"
     ]
    }
   ],
   "source": [
    "# List of concepts\n",
    "Concepts = get_concepts_list() \n",
    "\n",
    "#Input concept\n",
    "concept = wn.synset( get_synset(\"airplane\") )\n",
    "\n",
    "# Number of closest concepts to take into account\n",
    "num_concepts_2 = 15 #20\n",
    "\n",
    "print \"\\nMETRICS BASED ON PATH\"\n",
    "print \"\\nNumber of concepts: \", num_concepts_2\n",
    "\n",
    "# PATH SIMILARITY\n",
    "Path_sim = Apply_sim_metric(wn.path_similarity, num_concepts_2, concept)\n",
    "print \"\\nWordNet path similarity: \", Path_sim\n",
    "\n",
    "# Leacock-Chodorow similarity\n",
    "LC_sim = Apply_sim_metric(wn.lch_similarity, num_concepts_2, concept ) \n",
    "print \"\\n\\nLeacock-Chodorow similarity: \", LC_sim\n",
    "\n",
    "# Wu-Palmer Similarity\n",
    "WUP_sim = Apply_sim_metric(wn.wup_similarity, num_concepts_2, concept )\n",
    "print \"\\n\\nWu-Palmer similarity: \", WUP_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting list of closest vectors (Information content-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Corpus Information content...\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "#semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "\n",
    "print \"METRICS BASED ON INFORMATION CONTENT\"\n",
    "print \"\\nNumber of concepts: \", num_concepts_2\n",
    "\n",
    "# Resnik similarity.\n",
    "Res_sim = Apply_sim_metric(wn.res_similarity, num_concepts_2, concept, brown_ic)\n",
    "print \"\\nResnick similarity (brown_ic): \", Res_sim\n",
    "#Res_sim2 = Apply_sim_metric(wn.res_similarity, 20, concept, semcor_ic)\n",
    "#print \"\\nResnick similarity (semcor_ic): \", Res_sim2\n",
    "\n",
    "# Jiang-Conrath similarity\n",
    "JC_sim = Apply_sim_metric(wn.jcn_similarity, num_concepts_2, concept, brown_ic)\n",
    "print \"\\n\\nJiang-Conrath similarity (brown_ic): \", JC_sim\n",
    "#JC_sim2 = Apply_sim_metric(wn.jcn_similarity, 20, concept, semcor_ic)\n",
    "#print \"\\nJiang-Conrath similarity (semcor_ic): \", JC_sim2\n",
    "\n",
    "# Lin similarity\n",
    "Lin_sim = Apply_sim_metric(wn.lin_similarity, num_concepts_2, concept, brown_ic)\n",
    "print \"\\n\\nLin similarity (brown_ic): \", Lin_sim\n",
    "#Lin_sim2 = Apply_sim_metric(wn.lin_similarity, 20, concept, semcor_ic)\n",
    "#print \"\\nLin similarity (semcor_ic): \", Lin_sim2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersections and unions\n",
    "In this following cell we obtain the intersection sets for the McRae and HDcomputing similarity name list. The goal is to compare which method is more similar to each of the similarity metrics.\n",
    "The union sets is going to be used for creating Surveys to be applied on humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating name sets\n",
    "HDC_names = set([x[0] for x in HDC_sim])\n",
    "DatSet_names = set([x[0] for x in DatSet_sim])\n",
    "Path_names = set([x[0] for x in Path_sim])\n",
    "LC_names = set([x[0] for x in LC_sim])\n",
    "WUP_names = set([x[0] for x in WUP_sim])\n",
    "Res_names = set([x[0] for x in Res_sim])\n",
    "JC_names = set([x[0] for x in JC_sim])\n",
    "Lin_names = set([x[0] for x in Lin_sim])\n",
    "\n",
    "# Intersection between HDC and Dataset\n",
    "union_1 = HDC_names.intersection(DatSet_names)\n",
    "\n",
    "# Union of intersecion Path and intersection IC...\n",
    "PathInt = Path_names.intersection(LC_names, WUP_names)\n",
    "ICInt = Res_names.intersection(JC_names, Lin_names)\n",
    "union_2 = PathInt.union(ICInt)\n",
    "\n",
    "# Intersection between all NLTK metrics\n",
    "union_3 = PathInt.intersection(ICInt)\n",
    "\n",
    "# Intersection all NLTLK metrics and HDC and Dataset\n",
    "union_4 = HDC_names.intersection(PathInt, ICInt)\n",
    "union_5 = DatSet_names.intersection(PathInt, ICInt)\n",
    "union_6 = HDC_names.intersection(DatSet_names, PathInt, ICInt)\n",
    "\n",
    "# Ultimate union...\n",
    "ult_union = set.union( union_1, union_2, union_3, union_4, union_5, union_6 )\n",
    "print \"**Ultimate Union** lenght: \", len(ult_union), \"\\n\\n\", ult_union\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous cell... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Intersection between HDC and Dataset\n",
    "union_1 = HDC_names.intersection(DatSet_names)\n",
    "print \"*1*HDC and Dataset: \", union_1\n",
    "\n",
    "# Intersection between HDC and NLTK similarity metrics\n",
    "print \"\\n\\nHDC & NLTK similarity metrics\"\n",
    "for sett in [Path_names, LC_names, WUP_names, Res_names, JC_names, Lin_names]:\n",
    "    print \"\\n\", HDC_names.intersection(sett)\n",
    "    \n",
    "# Intersection between HDC and NLTK similarity metrics\n",
    "print \"\\n\\nDataset & NLTK similarity metrics\"\n",
    "for sett in [Path_names, LC_names, WUP_names, Res_names, JC_names, Lin_names]:\n",
    "    print \"\\n\", DatSet_names.intersection(sett)\n",
    "\n",
    "# Intersection between Path based metrics\n",
    "print \"\\n\\nIntersection Path based metrics\"\n",
    "PathInt = Path_names.intersection(LC_names, WUP_names)\n",
    "print \"\\n\", PathInt, \"...\", len(PathInt) / float( num_concepts_2 ) * 100, \"%\"\n",
    "\n",
    "# Intersection between IC based metrics\n",
    "print \"\\n\\nIntersection IC based metrics\"\n",
    "ICInt = Res_names.intersection(JC_names, Lin_names)\n",
    "print \"\\n\", ICInt\n",
    "\n",
    "# Union of intersecion Path and intersection IC...\n",
    "print \"\\n\\n*2*Union of intersections\"\n",
    "union_2 = PathInt.union(ICInt)\n",
    "print \"\\n\", union_2\n",
    "\n",
    "# Intersection between all NLTK metrics\n",
    "print \"\\n\\n*3*Intersection between all NLTK metrics\"\n",
    "union_3 = PathInt.intersection(ICInt)\n",
    "print \"\\n\", union_3\n",
    "\n",
    "# Intersection all NLTLK metrics and HDC and Dataset\n",
    "print \"\\n\\n*4*Intersection between NLTK metrics and HDC\"\n",
    "union_4 = HDC_names.intersection(PathInt, ICInt)\n",
    "print \"\\nHDC: \", union_4\n",
    "union_5 = DatSet_names.intersection(PathInt, ICInt)\n",
    "print \"\\n*5*Dataset: \", union_5\n",
    "union_6 = HDC_names.intersection(DatSet_names, PathInt, ICInt)\n",
    "print \"\\n*6*ALL: \", union_6\n",
    "\n",
    "\n",
    "# Ultimate union...\n",
    "ult_union = set.union( union_1, union_2, union_3, union_4, union_5, union_6 )\n",
    "print \"**Ultimate Union** lenght: \", len(ult_union), \"\\n\", ult_union\n",
    "\n",
    "# Union for all names...\n",
    "Union = set().union(HDC_names, DatSet_names, Path_names, LC_names, WUP_names, Res_names, JC_names, Lin_names)\n",
    "print \"\\n\\nUnion lenght: \", len(Union), \"\\n\", Union"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
