{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing similarity measures\n",
    "\n",
    "In this notebook a comparison between experimental results on humans and different similarity metrics are compared. \n",
    "This version uses dictionaries to avoid running the program many times.\n",
    "\n",
    "# PENDING...\n",
    "\n",
    " - Normalizar todo.. \n",
    " - Respaldar en github...\n",
    " - Hacer comparaciones contra individuos especificos de la encuesta (mi respuesta y otras que parezcan coherentes..)\n",
    "   (PARA ESO debo de MARCAR las preguntas que eliminé...)\n",
    " - \"Normalizar\" datos, en vez de 1 - 6 pasar de 0 a 1... \n",
    " - Hacer un conjunto de prueba hecho por mi, de 20 parejas\n",
    " - Comenzar a añadir 'la otra forma' de codificar... hacer algunas pruebas básicas... \n",
    " - Viernes enseñar resultados a Barrón y preguntarle si añado lo de la otra forma de codificar... \n",
    "\n",
    "\n",
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from scipy.stats.stats import pearsonr   \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('wordnet_ic')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "\n",
    "exp_file = 'ExperimentalResults_2.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting pairs of concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['spoon', 'barrel'], ['shoes', 'bike'], ['spoon', 'truck'], ['chair', 'pen'], ['spoon', 'envelope'], ['shoes', 'whip'], ['spoon', 'box'], ['shoes', 'shield'], ['chair', 'book'], ['table', 'brush'], ['spoon', 'basket'], ['table', 'curtains'], ['spoon', 'ashtray'], ['table', 'barrel'], ['chair', 'car'], ['table', 'lamp'], ['carpet', 'scarf'], ['shoes', 'carpet'], ['bed', 'lamp'], ['bed', 'mink_(coat)'], ['table', 'knife'], ['chair', 'closet'], ['shoes', 'earmuffs'], ['chair', 'dresser'], ['table', 'spoon'], ['bed', 'curtains'], ['shoes', 'skirt'], ['chair', 'bookcase'], ['shoes', 'mink_(coat)'], ['table', 'bookcase'], ['bowl', 'bathtub'], ['bed', 'chair'], ['bed', 'closet'], ['spoon', 'colander'], ['shoes', 'belt'], ['bed', 'dresser'], ['table', 'sofa'], ['bed', 'table'], ['boots', 'belt'], ['pen', 'envelope'], ['bed', 'pajamas'], ['stove', 'pot'], ['table', 'bench'], ['bed', 'cushion'], ['shovel', 'machete'], ['table', 'chair'], ['spoon', 'tongs'], ['bed', 'sofa'], ['spoon', 'spatula'], ['spoon', 'plate'], ['bed', 'pillow'], ['knife', 'scissors'], ['shoes', 'socks'], ['table', 'desk'], ['cup', 'bottle'], ['skirt', 'trousers'], ['chair', 'sofa'], ['shoes', 'sandals'], ['chair', 'bench'], ['spoon', 'fork'], ['shoes', 'slippers'], ['chair', 'rocker'], ['spoon', 'ladle'], ['shoes', 'boots']]\n"
     ]
    }
   ],
   "source": [
    "def ListofPairs (number):\n",
    "    \"It obtains a list of pairs of concepts\"\n",
    "    df = pd.read_excel(exp_file)\n",
    "    if number > 0:\n",
    "        string = 'Q' + str(number)    \n",
    "    else:\n",
    "        string = 'average'\n",
    "    # 1) List of concepts\n",
    "    ordered = df.sort_values(by=string)\n",
    "    c1 = map(str, list( ordered['concept 1'] ))\n",
    "    c2 = map(str, list( ordered['concept 2'] ))\n",
    "    L1 = map(list, zip(c1,c2))\n",
    "    # 2) Human similarity\n",
    "    ordered = df.sort_values(by=string)\n",
    "    L2 = map(lambda x: round(float(x), 3) / 10, list(ordered[string]))\n",
    "    \n",
    "    return L1, L2\n",
    "\n",
    "PConcepts, Hum_Sim = ListofPairs(0)\n",
    "print PConcepts\n",
    "#Hum_Sim = HumanSim()\n",
    "# Plotting\n",
    "#plt.plot(Hum_Sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HD Computing similarity\n",
    "\n",
    "### Initializing memory and encoding dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of initialization\n",
      "End of encoding\n"
     ]
    }
   ],
   "source": [
    "%run KB_HDComputing.ipynb\n",
    "\n",
    "# Initializing Memory\n",
    "Init_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing pairs of concepts in the HD binary space\n",
    "\n",
    "To make computations more efficient I created a dictionary where the similarity between a pair of concepts is stored. Everytime that a similarity is required the only thing to do is to consult this dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance vector\n",
    "HD_sim = []\n",
    "for cc in PConcepts:\n",
    "    HD_sim.append( HDvector.dist(Dict[cc[0]].getPointer(), Dict[cc[1]].getPointer()) )\n",
    "#Normalizing...\n",
    "HD_sim = map(lambda x: round(1. - x/float( max(HD_sim)) , 3), HD_sim)\n",
    "\n",
    "# Dictionary of distances\n",
    "Dict_HD = {}\n",
    "for key in range(len(HD_sim)):\n",
    "    Dict_HD[key] = HD_sim[key]\n",
    "\n",
    "# Plotting\n",
    "# plt.plot(HD_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## McRae similarity\n",
    "\n",
    "The following cells consult the similarity for each pair of concepts in the distance matrix provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def McRae_simi (pair_concepts):\n",
    "    \"Given a pair of concepts (in a list) it consults the similarity from the cos_matrix... file\"\n",
    "    try: \n",
    "        df = pd.read_excel(pathh + 'cos_matrix_brm_IFR.xlsx','1st_200')\n",
    "        return list(df.loc[df['CONCEPT'] == pair_concepts[0]][pair_concepts[1]])[0]\n",
    "    except:\n",
    "        try:\n",
    "            df = pd.read_excel(pathh + 'cos_matrix_brm_IFR.xlsx','2nd_200')\n",
    "            return list(df.loc[df['CONCEPT'] == pair_concepts[0]][pair_concepts[1]])[0]\n",
    "        except:\n",
    "            df = pd.read_excel(pathh + 'cos_matrix_brm_IFR.xlsx','last_141')\n",
    "            return list(df.loc[df['CONCEPT'] == pair_concepts[0]][pair_concepts[1]])[0]\n",
    "\n",
    "\n",
    "# Dictionary of distances (McRae)\n",
    "Dict_McRae = {}\n",
    "key = 0\n",
    "for cc in PConcepts:\n",
    "    Dict_McRae[key] = McRae_simi(cc)\n",
    "    key += 1\n",
    "\n",
    "#plt.plot( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NTLK Library functions\n",
    "\n",
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "def get_synset (concept):\n",
    "    \"Given a concept name (string) it returns its synset (string)\"\n",
    "    # Dataframe for excel document\n",
    "    df = pd.read_excel(pathh + 'CONCS_Synset_brm.xlsx') #../McRaedataset/CONCS_Synset_brm.xlsx')\n",
    "    row = df.loc[df['Concept'] == concept]\n",
    "    return str(list(row['Synset'] )[0])\n",
    "\n",
    "def similarity_fun ( similarity_metric, pair, corpus = None):\n",
    "    \"Given a similarity_metric function it returns a list of the num closest concepts to 'concept'\"\n",
    "    c_synset_1 = wn.synset( get_synset(pair[0]))\n",
    "    c_synset_2 = wn.synset( get_synset(pair[1]))\n",
    "    if corpus:\n",
    "        return round(similarity_metric(c_synset_1, c_synset_2, corpus), 3)\n",
    "    else:\n",
    "        return round(similarity_metric(c_synset_1, c_synset_2), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path similarity\n",
    "Dict_path = {}\n",
    "Dict_lch = {}\n",
    "Dict_wup = {}\n",
    "Dict_res = {}\n",
    "Dict_jcn = {}\n",
    "Dict_lin = {}\n",
    "\n",
    "key = 0\n",
    "for pair in PConcepts:\n",
    "    Dict_path[key] = similarity_fun(wn.path_similarity, pair)\n",
    "    Dict_lch[key] = similarity_fun(wn.lch_similarity, pair)\n",
    "    Dict_wup[key] = similarity_fun(wn.wup_similarity, pair)\n",
    "    Dict_res[key] = similarity_fun(wn.res_similarity, pair, brown_ic)\n",
    "    Dict_jcn[key] = similarity_fun(wn.jcn_similarity, pair, brown_ic)\n",
    "    Dict_lin[key] = similarity_fun(wn.lin_similarity, pair, brown_ic)\n",
    "    key += 1\n",
    "    \n",
    "#plt.plot(Path_sim) #plt.plot(LC_sim) #plt.plot(WUP_sim) #plt.plot(Res_sim) #plt.plot(JC_sim) #plt.plot(Lin_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations\n",
    "\n",
    "Using correlation to compare obtained similarity values from different metrics.\n",
    "Loop to obtain correlations between each questionary (and the average) and each similarity metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q3', 0.5917556237420544, ['HDC', 0.6836631499222855], ['res', 0.6577614906127348], ['McRae', 0.6359506244359471], ['jcn', 0.6154477235373255], ['path', 0.5562118840318038], ['lin', 0.5491882602484144], ['lch', 0.5271583062509282], ['wup', 0.508663550896996]]\n",
      "['Q24', 0.5361960095408826, ['HDC', 0.7475895935720432], ['McRae', 0.6536098717250649], ['jcn', 0.5565282522026359], ['res', 0.5326522166829488], ['path', 0.5124958597377782], ['lin', 0.4437791163246515], ['lch', 0.44339373603528853], ['wup', 0.39951943004664997]]\n",
      "['Q25', 0.47116070895639545, ['HDC', 0.6927696532417578], ['McRae', 0.6399808913880194], ['jcn', 0.4581763074636498], ['res', 0.44663941701157794], ['lin', 0.4089437785521335], ['path', 0.4076076535268331], ['lch', 0.3707731502648828], ['wup', 0.34439482020230894]]\n",
      "['Q30', 0.47116070895639545, ['HDC', 0.6927696532417578], ['McRae', 0.6399808913880194], ['jcn', 0.4581763074636498], ['res', 0.44663941701157794], ['lin', 0.4089437785521335], ['path', 0.4076076535268331], ['lch', 0.3707731502648828], ['wup', 0.34439482020230894]]\n",
      "['Q11', 0.4424541950853719, ['jcn', 0.6180961294405283], ['HDC', 0.5010891380947233], ['res', 0.4518378963817116], ['path', 0.44434027075727306], ['lin', 0.40710257351959867], ['McRae', 0.39793935124858004], ['lch', 0.3846194391767978], ['wup', 0.33460876206376267]]\n",
      "['Q5', 0.4598204498313924, ['HDC', 0.735674987162895], ['McRae', 0.6786854055375082], ['jcn', 0.464380842112284], ['res', 0.43372519604485044], ['path', 0.3995501428843089], ['lin', 0.34888857045328836], ['lch', 0.32808278065295804], ['wup', 0.28957567380304633]]\n",
      "['Q0', 0.47157344656921835, ['HDC', 0.8006728271653207], ['McRae', 0.7138997493082799], ['jcn', 0.4784482047797414], ['res', 0.4683139846329326], ['path', 0.3624448743565741], ['lin', 0.36087238780369063], ['lch', 0.3082778276974036], ['wup', 0.27965771680980356]]\n",
      "['Q21', 0.3940164485638311, ['HDC', 0.6828525038530531], ['McRae', 0.5666839011935534], ['res', 0.42483913851460064], ['jcn', 0.33403006723055767], ['lin', 0.33016947779417904], ['path', 0.27574981668635146], ['lch', 0.2708269085991255], ['wup', 0.26697977463922845]]\n",
      "['Q26', 0.40805626919587457, ['HDC', 0.7108331261344647], ['McRae', 0.603760030288089], ['res', 0.41498603485342966], ['jcn', 0.3735432545604623], ['path', 0.32419668049923406], ['lch', 0.29016144589245985], ['lin', 0.2892534754299396], ['wup', 0.25771610590891675]]\n",
      "['Q4', 0.4480910780874233, ['HDC', 0.7349020314939381], ['McRae', 0.6455839611849384], ['jcn', 0.5078925830088648], ['res', 0.4313593801038358], ['lin', 0.3693239682822185], ['path', 0.3558785974283805], ['lch', 0.289970452857832], ['wup', 0.24981765033937836]]\n",
      "['Q20', 0.41734315963617424, ['McRae', 0.6758511938086206], ['HDC', 0.6671649323745271], ['jcn', 0.5123562728681436], ['lin', 0.3855421569845399], ['res', 0.3549669055127945], ['path', 0.2685686390840096], ['lch', 0.2419571510221835], ['wup', 0.23233802543457507]]\n",
      "['Q9', 0.4256996484231341, ['HDC', 0.7754494361729509], ['McRae', 0.6833085497340585], ['jcn', 0.4714803906864961], ['res', 0.3638244370812098], ['path', 0.3440421234647613], ['lin', 0.2937809874452435], ['lch', 0.26536687098483386], ['wup', 0.20834439181551895]]\n",
      "['Q7', 0.3504528830106268, ['McRae', 0.6043378342936218], ['HDC', 0.592161797159454], ['jcn', 0.37648452765432433], ['res', 0.3361143803979102], ['path', 0.2797139700229297], ['lin', 0.2294705929396589], ['lch', 0.21115806042596708], ['wup', 0.1741819011911486]]\n",
      "['Q18', 0.35591041703166165, ['HDC', 0.590370795017711], ['McRae', 0.5791788953636348], ['jcn', 0.47568726946362544], ['res', 0.33845723901258673], ['lin', 0.28919584971761547], ['path', 0.21561790576098844], ['lch', 0.18523263982150848], ['wup', 0.1735427420956228]]\n",
      "['Q17', 0.30908020762404387, ['HDC', 0.6494735929450093], ['McRae', 0.5350762552782452], ['res', 0.30664601700111627], ['jcn', 0.2648953085977003], ['path', 0.2003988425035571], ['lin', 0.18329290870158804], ['lch', 0.17485084803958356], ['wup', 0.15800788792555132]]\n",
      "['Q13', 0.3337129478132375, ['McRae', 0.6249213545511516], ['HDC', 0.6110682281966757], ['jcn', 0.3560187575683178], ['res', 0.3011084673267942], ['path', 0.2512003490987392], ['lin', 0.21435123616994792], ['lch', 0.17343847583980587], ['wup', 0.13759671375446794]]\n",
      "['Q14', 0.3337129478132375, ['McRae', 0.6249213545511516], ['HDC', 0.6110682281966757], ['jcn', 0.3560187575683178], ['res', 0.3011084673267942], ['path', 0.2512003490987392], ['lin', 0.21435123616994792], ['lch', 0.17343847583980587], ['wup', 0.13759671375446794]]\n",
      "['Q2', 0.3019170613246238, ['HDC', 0.523612214403167], ['McRae', 0.4762798305754507], ['jcn', 0.33915994170145225], ['res', 0.3139544672969081], ['lin', 0.2655922357051906], ['path', 0.2038503619544626], ['lch', 0.16164607406664083], ['wup', 0.13124136489371818]]\n",
      "['Q22', 0.29066176136639604, ['HDC', 0.5859434900860705], ['McRae', 0.5420513893151659], ['jcn', 0.40859858007456773], ['res', 0.1942290093620494], ['lin', 0.18357219686954246], ['path', 0.1810344667785805], ['lch', 0.12949442086792382], ['wup', 0.10037053757726802]]\n",
      "['Q10', 0.2784725905412574, ['HDC', 0.7133075139756442], ['McRae', 0.5901324986927544], ['jcn', 0.3136107904365882], ['res', 0.23042440040831824], ['path', 0.1476492211433695], ['lin', 0.13411478255593604], ['lch', 0.06555702735185487], ['wup', 0.03298448976559394]]\n",
      "['Q15', 0.15016293118767307, ['HDC', 0.6126676544551894], ['McRae', 0.46148472531002854], ['jcn', 0.06794430001881714], ['path', 0.059797070588221514], ['res', 0.04890078086214067], ['lch', -0.000544211168776849], ['lin', -0.013202213681767992], ['wup', -0.03574465688246784]]\n",
      "['Q27', 0.11133777482331633, ['McRae', 0.4407910087767634], ['HDC', 0.41545983188523017], ['jcn', 0.10522105309634656], ['res', 0.05535089938482107], ['lin', 0.00495489790639054], ['path', -0.014008973343151676], ['lch', -0.04932004983733389], ['wup', -0.06774646928253555]]\n",
      "['Q23', 0.47222913510338504, ['HDC', 0.5668376699442366], ['res', 0.5350962803127656], ['McRae', 0.5022504997756353], ['lin', 0.4731692512854516], ['jcn', 0.4641838150689946], ['wup', 0.4304580084085939], ['lch', 0.40334620879123156], ['path', 0.402491347240171]]\n",
      "['Q12', 0.19544314601749366, ['HDC', 0.34602372597298275], ['McRae', 0.3069201494838407], ['res', 0.29017605933145285], ['lin', 0.2107536926998843], ['jcn', 0.15363296102861707], ['wup', 0.1066420852494855], ['lch', 0.07735104714052704], ['path', 0.07204544723315894]]\n",
      "['Q6', 0.4398057201978857, ['HDC', 0.6850863804597909], ['McRae', 0.6380132398018988], ['jcn', 0.47370301510190366], ['res', 0.4101137097368982], ['path', 0.35684569303845287], ['lch', 0.3292143135328278], ['wup', 0.31647417171854797], ['lin', 0.30899523819276553]]\n",
      "['Q28', 0.368982110007243, ['HDC', 0.637342040387959], ['McRae', 0.5714051755134276], ['res', 0.4222418888181898], ['jcn', 0.28996819178530114], ['path', 0.2887844056083111], ['lch', 0.2677724775540962], ['wup', 0.25571431728669064], ['lin', 0.2186283831039686]]\n",
      "['Q29', 0.368982110007243, ['HDC', 0.637342040387959], ['McRae', 0.5714051755134276], ['res', 0.4222418888181898], ['jcn', 0.28996819178530114], ['path', 0.2887844056083111], ['lch', 0.2677724775540962], ['wup', 0.25571431728669064], ['lin', 0.2186283831039686]]\n",
      "['Q16', 0.23063332308310897, ['HDC', 0.32112791332311413], ['McRae', 0.30920734975823055], ['jcn', 0.22833934858764818], ['res', 0.21502479341419448], ['path', 0.20914943242287598], ['lch', 0.201997250894263], ['wup', 0.19412511002313582], ['lin', 0.16609538624140954]]\n",
      "['Q19', 0.17541456400403135, ['HDC', 0.5530994331745696], ['McRae', 0.5035017284074778], ['res', 0.1456800189812724], ['jcn', 0.13283398332206603], ['path', 0.061570279708654155], ['lch', 0.02438193677818998], ['wup', -0.004703301370811109], ['lin', -0.01304756696916795]]\n",
      "['Q1', 0.4795014882560561, ['HDC', 0.699515199623018], ['McRae', 0.6075957622790952], ['res', 0.5438918474438033], ['jcn', 0.4625221973275926], ['lin', 0.4256549705880452], ['path', 0.37093344311904247], ['wup', 0.3633213165879377], ['lch', 0.3625771690799143]]\n",
      "['Q8', 0.2281730870068706, ['HDC', 0.5497610753344617], ['McRae', 0.46191769306451463], ['jcn', 0.3247101757899046], ['lin', 0.22503484871365667], ['res', 0.21853079674753093], ['path', 0.04528245432068036], ['wup', 0.0013339201467724821], ['lch', -0.0011862680625563738]]\n"
     ]
    }
   ],
   "source": [
    "Corr_mat = []\n",
    "for Q in range(31):\n",
    "    # 1) Obtain \"human list\", it can be the average responses or an specific one\n",
    "    Hum_sim =  ListofPairs(Q) #or Q + #1-30\n",
    "    \n",
    "    # 2) Obtain list of keys for previous list\n",
    "    keys = map(lambda x: PConcepts.index(x), Hum_sim[0])\n",
    "    \n",
    "    # 3) Create a list of distances according to each metric by consulting the appropiate dictionary\n",
    "    HD_sim = [Dict_HD[x] for x in keys]\n",
    "    McRae_sim = [Dict_McRae[x] for x in keys]\n",
    "    path_sim = [Dict_path[x] for x in keys]\n",
    "    lch_sim = [Dict_lch[x] for x in keys]\n",
    "    # optional... \n",
    "    lch_sim = map(lambda x: round(x/float( max(lch_sim)) , 3), lch_sim)\n",
    "    \n",
    "    wup_sim = [Dict_wup[x] for x in keys]\n",
    "    res_sim = [Dict_res[x] for x in keys]\n",
    "    # optional\n",
    "    res_sim = map(lambda x: round(x/float( max(res_sim)) , 3), res_sim)\n",
    "    \n",
    "    jcn_sim = [Dict_jcn[x] for x in keys]\n",
    "    lin_sim = [Dict_lin[x] for x in keys]\n",
    "\n",
    "    # 4) Calculate correlations\n",
    "    correlations = [['HDC', pearsonr(Hum_sim[1], HD_sim)[0]], ['McRae', pearsonr(Hum_sim[1], McRae_sim)[0]],\n",
    "                    ['path', pearsonr(Hum_sim[1], path_sim)[0]], ['lch', pearsonr(Hum_sim[1], lch_sim)[0]],\n",
    "                    ['wup', pearsonr(Hum_sim[1], wup_sim)[0]], ['res', pearsonr(Hum_sim[1], res_sim)[0]],\n",
    "                    ['jcn', pearsonr(Hum_sim[1], jcn_sim)[0]], ['lin', pearsonr(Hum_sim[1], lin_sim)[0]]]\n",
    "\n",
    "    # 5) Sort list so far (key = )\n",
    "    correlations = sorted(correlations, key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    # 6) Adding 'name' and average correlation value\n",
    "    Corr_mat.append( ['Q' + str(Q), np.mean([r[1] for r in correlations])] + correlations )\n",
    "\n",
    "# 7) Sorting entire matrix by average value (last...)\n",
    "Corr_mat = sorted(Corr_mat, key = lambda x: x[-1], reverse = True)\n",
    "\n",
    "# Printing\n",
    "for v in Corr_mat:\n",
    "    print v\n",
    "    \n",
    "# FALTA NORMALIZAR TODAS... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[0, 1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "a = [0]\n",
    "l = [1,2,3,4, 5]\n",
    "print a.extend(l)\n",
    "print a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
